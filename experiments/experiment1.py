import os
import re
import glob
import json
import argparse
import uuid
from pprint import pprint
from distutils.util import strtobool
from datetime import datetime
from itertools import product
from collections import defaultdict
import pathlib

import numpy as np

from utils import dump_results, dump_pickle, load_pickle, TIMESTAMP_FORMAT#
import generate_appm
import sampling
import recovery

pathlib.Path('./data/experiment1/graphs').mkdir(parents=True, exist_ok=True)
pathlib.Path('./data/experiment1/samples').mkdir(parents=True, exist_ok=True)
pathlib.Path('./data/experiment1/recovery').mkdir(parents=True, exist_ok=True)

def bool_type(x):
  return bool(strtobool(x))

DEFAULT_SAMPLING_METHOD = "RandomWalkSampling"

def parse_graph_generate_args():
  parser = argparse.ArgumentParser("Experiment 1: Graph generation")

  parser.add_argument("--seed", type=int, default=None, help="Random seed")

  parser.add_argument("--p",
                      type=float,
                      default=3e-1,
                      help=("Parameter p for assortative planted partition"
                            " model (APPM), specifying the probability that"
                            " two nodes i,j out of the same cluster are"
                            " connected by an edge"))

  parser.add_argument("--q",
                      type=float,
                      default=5e-2,
                      help=("Parameter q for assortative planted partition"
                            " model (APPM), specifying the probability that"
                            " two nodes i,j out of two different clusters are"
                            " connected by an edge"))

  parser.add_argument("--cluster_sizes",
                      type=int,
                      nargs='+',
                      default=[10, 20, 30, 40],
                      help=("Parameter p for assortative planted partition"
                            " model (APPM), specifying the probability that"
                            " two nodes i,j out of two different clusters are"
                            " connected by an edge"))

  parser.add_argument("--num_graphs",
                      type=int,
                      default=100,
                      help="Number of graphs to generate")

  parser.add_argument("--cull_disconnected",
                      action="store_true",
                      dest="cull_disconnected",
                      default=True,
                      help="Cull nodes that are not connected to the main"
                           " graph")

  parser.add_argument("--no_cull_disconnected",
                      action="store_false",
                      dest="cull_disconnected",
                      help="Use to leave nodes that are not connected to the"
                           " main graph")

  parser.add_argument("--connect_disconnected",
                      action="store_true",
                      dest="connect_disconnected",
                      help="Connect nodes that are not connected to the main"
                           " graph")

  parser.add_argument("--no_connect_disconnected",
                      action="store_false",
                      dest="connect_disconnected",
                      help="Connect nodes that are not connected to the main"
                           " graph")

  parser.set_defaults(cull_disconnected=True)
  parser.set_defaults(connect_disconnected=False)

  generator_types = list(generate_appm.SIGNAL_GENERATORS.keys())
  parser.add_argument("--generator_type",
                      type=str,
                      default="uniform",
                      help="Type of signal generator to use for cluster"
                           " values. Choose from"
                           " {}".format(generator_types))

  parser.add_argument("--results_base",
                      default="./data/experiment1/graphs",
                      type=str,
                      help="Directory to write graphs to.")

  args, unknown = parser.parse_known_args()

  return vars(args)

def parse_sampling_args():
  parser = argparse.ArgumentParser("Experiment 1: Graph sampling")

  parser.add_argument("--seed", type=int, default=None, help="Random seed")

  parser.add_argument("--Ls",
                      type=int,
                      nargs='+',
                      default=[20],
                      help=("Values for random walk length L to run experiments"
                            " for. The complete parameter space to run the"
                            " experiments for is generated by taking cartesian"
                            " product of Ls and other parameters (Ms)"))

  parser.add_argument("--Ms",
                      type=int,
                      nargs='+',
                      default=[10],
                      help=("Values for sampling budget M to run experiments"
                            " for. The complete parameter space to run the"
                            " experiments for is generated by taking cartesian"
                            " product of Ms and other parameters (Ls)"))


  parser.add_argument("--graphs_file_pattern",
                      type=str,
                      default=("./data/experiment1/graphs/"
                               f"{datetime.now().strftime('%Y%m%d')}*.pk"),
                      help=("Run experiment for the graph files matching the"
                            " file pattern. The pattern is passed to glob.iglob"
                            " function. Each file can contain multiple graphs"
                            " in an array. See experiment1 graph generate step"
                            " for details."))

  parser.add_argument("--sampling_method",
                      type=str,
                      default=sampling.DEFAULT_SAMPLING_METHOD,
                      help=("Name of the class used for graph sampling. The"
                            " sampling class must be importable from"
                            " algorithms.sampling module. Defaults to '{0}',"
                            " i.e. algorithms.sampling.{0}."
                            "".format(sampling.DEFAULT_SAMPLING_METHOD)))

  parser.add_argument("--results_base",
                      default="./data/experiment1/samples",
                      type=str,
                      help="Directory to write graphs to.")

  args, unknown = parser.parse_known_args()

  return vars(args)


def parse_recovery_args():
  parser = argparse.ArgumentParser("Experiment 1: Graph recovery")

  parser.add_argument("--seed", type=int, default=None, help="Random seed")

  parser.add_argument("--graphs_path",
                      type=str,
                      default="./data/experiment1/graphs/",
                      help="Define the folder path where to look for graph.")

  parser.add_argument("--samples_path",
                      type=str,
                      default="./data/experiment1/samples/",
                      help="Define the folder path where to look for samples.")

  parser.add_argument("--file_pattern",
                      type=str,
                      default=f"{datetime.now().strftime('%Y%m%d')}*.pk",
                      help=("File pattern to look for graphs and samples in"
                            " their respective paths."))

  parser.add_argument("--recovery_method",
                      type=str,
                      default=recovery.DEFAULT_RECOVERY_METHOD,
                      help=("Name of the class used for graph recovery. The"
                            " recovery class must be importable from"
                            "algorithms.recovery module. Defaults to '{0}',"
                            " i.e. algorithms.recovery.{0}."
                            "".format(recovery.DEFAULT_RECOVERY_METHOD)))

  parser.add_argument("--results_base",
                      default="./data/experiment1/recovery",
                      type=str,
                      help="Directory to write results to.")

  args, unknown = parser.parse_known_args()

  return vars(args)


ARGS_PARSERS = {
  "graph_generate": parse_graph_generate_args,
  "sampling": parse_sampling_args,
  "recovery": parse_recovery_args,
}

def parse_args():
  parser = argparse.ArgumentParser(
    description=(
      "Experiment 1 based on:\n"
      "Basirian, S. and Jung, A., 2017."
      " Random Walk Sampling for Big Data over Networks."
      " arXiv preprint arXiv:1704.04799."))

  parser.add_argument("-v", "--verbose",
                      type=bool_type,
                      default=False,
                      help="Verbose")

  # TODO: add choices for --step
  parser.add_argument("--step",
                      type=str,
                      default="graph_generate",
                      help="Step of the pipeline to run")

  args, unknown = parser.parse_known_args()

  return vars(args)

def run_graph_generate(args):
  print("graph_generate")
  generate_args_base = {
    "sizes": args["cluster_sizes"],
    "p_in": args["p"],
    "p_out": args["q"],
    "seed": args["seed"],
    "cull_disconnected": args["cull_disconnected"],
    "connect_disconnected": args["connect_disconnected"],
    "generator_type": args["generator_type"],
    "out_path": None,
    "visualize": False,
  }

  graphs = {}
  num_graphs = args["num_graphs"]
  for i in range(num_graphs):
    if i == 0 or (i+1) % 100 == 0 or i == (num_graphs - 1):
      print(f"graph {i+1}/{num_graphs}")

    generate_args = generate_args_base.copy()

    if args.get('verbose', False):
      print(f"{i}: {generate_args}")

    graph = generate_appm.main(generate_args)
    graph_id = str(uuid.uuid4())
    graphs[graph_id] = {'args': generate_args, 'graph': graph }

  out_path = (f"{args['results_base']}"
              f"/{datetime.now().strftime(TIMESTAMP_FORMAT)}.pk")

  dump_pickle(graphs, out_path)

def run_sampling(args):
  print("sampling")

  sampling_args_base = { "seed": args["seed"] }

  num_files = len(glob.glob(args["graphs_file_pattern"]))
  for i, filepath in enumerate(glob.iglob(args["graphs_file_pattern"])):
    print(f"sampling graphs from file {i+1}/{num_files}")
    graphs_data = load_pickle(filepath)
    num_graphs = len(graphs_data)

    sampling_result = defaultdict(list)
    for j, (graph_id, graph_data) in enumerate(graphs_data.items()):
      if j == 0 or (j+1) % 100 == 0 or j == (num_graphs - 1):
        print(f"graph {j+1}/{num_graphs}")

      graph = graph_data['graph']
      graph_args = graph_data['args']

      for L, M in product(args["Ls"], args["Ms"]):
        sampling_args = sampling_args_base.copy()
        sampling_params = { "L": L, "M": M }
        sampling_args.update({
          "graph": graph,
          "sampling_method": args["sampling_method"],
          "sampling_params": sampling_params,
          # "results_file": (f"{args['results_base']}"
          #                  f"/{L}-{M}-{filepath.split('/')[-1]}")
        })

        if args.get('verbose', False):
          print(f"{sampling_args}")

        samples = sampling.main(sampling_args)['sampling_set']

        sampling_result[graph_id].append(
          {
            "sampling_method": args["sampling_method"],
            "sampling_params": sampling_params,
            "samples": samples
          }
        )


    filename = filepath.split('/')[-1]
    out_path = f"{args['results_base']}/{filename}"

    dump_pickle(sampling_result, out_path)

def graph_filepath_from_sample_filepath(sample_filepath):
  parts = sample_filepath.split("/")
  graph_folder = "/".join(parts[:-1]).replace("samples", "graphs")
  filename_match = re.search("(\d+-\d+)-(\d+.json)", sample_filepath)
  graph_filename = filename_match.group(2)
  graph_filepath = f"{graph_folder}/{graph_filename}"

  return graph_filepath

def run_recovery(args):
  print("recovery")

  recovery_args_base = {
    "recovery_method": args["recovery_method"],
  }

  graphs_path = args["graphs_path"]
  samples_path = args["samples_path"]
  file_pattern = args["file_pattern"]
  graph_files = glob.glob(os.path.join(graphs_path, file_pattern))
  sample_files = glob.glob(os.path.join(samples_path, file_pattern))
  assert(len(graph_files) == len(sample_files))

  num_files = len(graph_files)
  for i, (graph_filepath, sample_filepath) in enumerate(zip(graph_files,
                                                            sample_files)):
    print(f"recovering graphs from file {i+1}/{num_files}")

    graphs_data = load_pickle(graph_filepath)
    samples_data = load_pickle(sample_filepath)

    recovery_results = defaultdict()

    num_graphs = len(graphs_data)
    for j, (graph_id, graph_data) in enumerate(graphs_data.items()):
      if j == 0 or (j+1) % 100 == 0 or j == (num_graphs - 1):
        print(f"graph {j+1}/{num_graphs}")

      graph = graph_data['graph']
      for sample_data in samples_data[graph_id]:
        samples = list(sample_data['samples'])

        recovery_args = recovery_args_base.copy()
        recovery_args.update({
          "graph": graph,
          "samples": samples,
          "recovery_params": {},
          # "results_file": (f"{args['results_base']}"
          #                  f"/{sample_filepath.split('/')[-1]}")
        })

        if args.get('verbose', False):
          print(f"{recovery_args}")

        result = recovery.main(recovery_args)
        sampling_params = sample_data['sampling_params']
        key = (graph_id, sampling_params['M'], sampling_params['L'])
        recovery_results[key] = result['nmse']

    filename = graph_filepath.split('/')[-1]
    out_path = f"{args['results_base']}/{filename}"

    dump_pickle(recovery_results, out_path)

def main(args):
  print(args)
  step = args["step"]
  if step == "graph_generate":
    run_graph_generate(args)
  elif step == "sampling":
    run_sampling(args)
  elif step == "recovery":
    run_recovery(args)

if __name__ == "__main__":
  args = parse_args()
  args.update(ARGS_PARSERS[args["step"]]())
  main(args)